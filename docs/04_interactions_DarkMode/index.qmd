---
title: "Statistical Models"
subtitle: "Lecture 4 - Interactions"
author: 
  - name: "Phil McAleer"
    email: "philip.mcaleer@glasgow.ac.uk"
    affiliation: "University of Glasgow"
format: 
  revealjs:
    theme: dark 
    code-line-numbers: false
    chalkboard: true
    mermaid: 
      theme: default
knitr:
  opts_chunk:
    echo: true
---

```{r}
#| label: setup
#| include: false
options(tidyverse.quiet=TRUE)
library("tidyverse")
library("lme4")

source("../theme_jetblack.R")

hgs <- readxl::read_excel("../data/Han_et_al.xlsx") |>
  mutate(ID = as.integer(ID)) |>
  select(ID, sex, HGS, F0)

## paste
.p <- paste0

## .fraction
.f <- function(x, y) {
  paste0("\\frac{", x, "}{", y, "}")
}

## y-bar
.yb1 <- function(x) {
  paste0("$\\bar{Y}_{", x, "}$")
}

.yb2 <- function(x) {
  paste0("\\bar{Y}_{", x, "}")
}

## subtraction term
.st <- function(x, y, bracket = NULL) {
  if (is.null(bracket)) {
    paste0(x, " - ", y)
  } else {
    paste0(bracket[1], x, " - ", y, bracket[2])
  }
}

.rb <- c("(", ")")
.dr <- c("\\displaystyle\\left(", "\\right)")
.ds <- c("\\displaystyle\\left[", "\\right]")
```

# Interactions

## Interactions

Last week we mentioned that in the models we were looking at we were considering the predictors as independent from each other - the effect of variable X on variable Y is not affected by variable Z

* the effect of lecture attendance on engagement is not affected by the number of clicks on the moodle webpage and vice versa. The variables work independently.

In reality, life is usually more complicated than that and variables often have some relationship or interaction with each other.


## Interactions

Leads to the notion of "It depends."

* Is a student's report grade impacted by their previous grade and tutorial engagement?
  * Some students might not need to engage as much if previous grade is high, others much get a big boost if previous grade is high and they attend lots of tutorials.
* [Interactions:]{style="color:yellow;"} The effect of a predictor variable on the response variable may depend upon the value(s) of one or more other predictor variables.*

## Two types of Interactions today: 

<br><br><br>

1. Continuous-by-Categorical Interactions (e.g. ANCOVA)
2. Categorical-by-Categorical Interactions (e.g. ANOVA)
3. Not Discussed: Continuous-by-Continuous Interactions

::: {.aside}
[ANCOVA = Analysis of Covariance; ANOVA = Analysis of Variance]{style="color:yellow;"}
:::

# Continuous-by-Categorical Interactions

## Continuous-by-Categorical Interactions {.smaller}

**Do stronger people tend to have lower voices?**

* Previous research has suggested a relationship between the pitch of a person's voice (F0 - f-zero, the fundamental frequency) and their strength. Lower pitch would indicate a stronger person. 
* Previous research has suggested this holds true in males and females (based on sex at birth)
* Three variables: **Outcome**: Hand-Grip Strength (HGS, continuous); **Predictors**: Sex (categorical), F0 (continuous)

::: aside
Han, C., Wang, H., Fasolt, V., Hahn, A. C., Holzleitner, I. J., Lao, J., DeBruine, L. M., Feinberg, D. R., & Jones, B. C. (2018). [No evidence for correlations between handgrip strength and sexually dimorphic acoustic properties of voices.](https://doi.org/10.1002/ajhb.23178). *American Journal of Human Biology*, *30*, e23178.

Data: <https://osf.io/na6be/>
:::

## The Data

```{r}
#| label: show-hgs
#| df-print: tibble
hgs <- readxl::read_excel("../data/Han_et_al.xlsx") |>
  mutate(ID = as.integer(ID)) |>
  select(ID, sex, HGS, F0)

hgs
```

---

```{r}
ggplot(hgs, aes(F0, HGS)) + 
  geom_point(colour = "yellow", size = 3, alpha = .4) +
  geom_smooth(method = "lm", se = FALSE, color = "orange") +
  labs(title = "Strong negative relationship between F0 & HGS") +
  theme_jetblack()
```


---

```{r}
lm(HGS ~ F0, hgs) |> summary() # unstandardised scores

```


```{r, echo=FALSE}
l1Out <- capture.output(summary(lm(HGS ~ F0, hgs)))
l1Out[16:18]
```

## Interpretation {.smaller}

* Intercept ($\beta_0$): 56.7
* Slope ($\beta_1$): -0.145
* Adjusted R-squared: .5672
* F(1,29) = 289.3, p < .001

Meaning:

* When pitch (F0) is equal to 0 then the predicted hand-grip strength is 56.7 kgf (kilogram-force)
* For every increase in pitch of 1 Hz, we expect a -0.145 decrease in HGS
* Significant model (relationship) explaining ~57% of variance, F(1,29) = 289.3, p < .001
* $\hat{HGS}$ = 56.7  + (-0.145 $\times$ F0)


##


```{r}
ggplot(hgs, aes(F0, HGS, color = sex)) + 
  geom_point(aes(shape = sex), size = 3, alpha = .6) + 
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_manual(values = c("orange", "green")) +
  labs(title = "Weak negative relationships between FO and HGS by Sex") +
  theme_jetblack()
```

## Adding sex of voice

* `SEX`: 0 = female, 1 = male

```{r}
# Dummy Code - though technically this is same coding R would do automatically
hgs2 <- hgs |>
  mutate(sex_male = if_else(sex == "male", 1, 0))
```

* double check with `distinct()`

```{r}
hgs2 |> distinct(sex, sex_male)
```

## Analysis {.smaller}

```{r, eval = FALSE}
lm(HGS ~ F0 * sex_male, hgs2) |> summary() 
```

* Note: `F0 * sex_male`, not `F0 + sex_male`
* the `*` is a shorthand notation for saying give me the individual effect of each predictor (called Main Effects) and the interaction between the predictors
* Long way of writing this would be: `F0 + sex + FO:sex`, with the `:` denoting the interaction.
* When only two predictors it can seem a bit obselete to use `*` but when you start getting beyond two predictors it says a lot of time.

---

```{r}
lm(HGS ~ F0 * sex_male, hgs2) |> summary() 
```


```{r, echo=FALSE}
l1Out <- capture.output(summary(lm(HGS ~ F0 * sex_male, hgs2)))
l1Out[18:20]
```

---


$$HGS_i = \beta_0 + \beta_1 F0_i + \beta_2 SEX_i + \beta_3 F0_i SEX_i + e_i$$

::: {.r-stack}
`HGS ~ F0 + sex + F0:sex`
:::

* $\beta_0$ (intercept): 29.76
* $\beta_1$ (F0-slope): -0.025
* $\beta_2$ (Sex-slope): 15.91
* $\beta_3$ (Interaction-slope): -0.016 
* $\hat{HGS}$ = 29.76 + (-0.025 $\times$ F0) + (15.91 $\times$ Sex) + (-0.016 $\times$ F0 $\times$ Sex)

## {.smaller}

* Keeping in mind that Sex is categorical as 0 = female and 1 = male

For females:

* $HGS_i = \beta_0 + \beta_1 F0_i + \beta_2 SEX_i + \beta_3 F0_i SEX_i + e_i$
* $\hat{HGS}$ = 29.76 + (-0.025 $\times$ F0) + (15.91 $\times$ Sex) + (-0.016 $\times$ F0 $\times$ Sex)
* $\hat{HGS}$ = 29.76 + (-0.025 $\times$ F0) + (15.91 $\times$ 0) + (-0.016 $\times$ F0 $\times$ 0)
* $\hat{HGS}$ = 29.76 + (-0.025 $\times$ F0) = $\beta_0 + \beta_1 F0_i$

Interpretation:

* Intercept value is value of HGS when sex and pitch are 0
* Because when sex is 0 it indicates female (in this situation)
* Intercept is the predicted HGS of female participants when pitch is 0; 29.76 kgf
* By same understanding, $\beta_1$ is the change in HGS when you increase female pitch by 1; HGS decreases by -0.025

## {.smaller}

For Males:

* $HGS_i = \beta_0 + \beta_1 F0_i + \beta_2 SEX_i + \beta_3 F0_i SEX_i + e_i$
* $\hat{HGS}$ = 29.76 + (-0.025 $\times$ F0) + (15.91 $\times$ Sex) + (-0.016 $\times$ F0 $\times$ Sex)
* $\hat{HGS}$ = 29.76 + (-0.025 $\times$ F0) + (15.91 $\times$ 1) + (-0.016 $\times$ F0 $\times$ 1)
* $\hat{HGS}$ = 29.76 + (-0.025 $\times$ F0) + 15.91 + (-0.016 $\times$ F0)

Which is the same as:

* $\hat{HGS}$ = 29.76 + 15.91 + (-0.025 + -0.016 $\times$ F0)
* $\hat{HGS} = \beta_0 + \beta_2 SEX_i + (\beta_1 + \beta_3 SEX_i) F0_i + e_i$
* Male Intercept = 29.76 + 15.91 + (-0.025 + -0.016 $\times$ 0) = 29.76 + 15.91 = 45.67
* Male slope = (-0.025 + -0.016) = -0.041

## Summary {.smaller}

Effectively have two formulas for predicting the categorical effect

- female: $\beta_0 + \beta_1 F0_i$
- male: $\beta_0 + \beta_2 + (\beta_1 + \beta_3) F0_i$

Betas can be interpreted as:

* $\beta_0$ (intercept): value of Y when category is 0 (e.g. female)
* $\beta_1$ (F0-slope): relationship of Y and X when category is 0 (e.g. female)
* $\beta_2$ (Sex-slope): the change in intercept to move from category 0 (e.g. female) to category 1 (male) - the offset from 0 to 1
* $\beta_3$ (Interaction-slope): the change in slope to move from category 0 (e.g. female) to category 1 (male) - the offset from 0 to 1

## Last point {.smaller}

```{r, echo=FALSE}
l1Out[10:14]
l1Out[18:20]
```

* Model is significant - F(3,217) = 116.2, p < .001
* Only the `sex_male` coefficient is significant, t(217) = 15.91, p = .045, indicating that the average male HGS is significantly bigger (M = 29.75 + 15.19 = 45.67) than the average female HGS (M = 29.75).
* Would conclude basically that there is a main effect of sex on HGS but no relationship of F0 on HGS and no interaction between F0 and sex on HGS
* Note: personally I find it easier to interpret if I put the continuous predictor into the model before the categorical predictor - F0 by sex as opposed to sex by F0 - as the betas map easier that way, as in example above - $\beta_0$ is first row, $\beta_1$ is second, etc....


# Categorical-by-Categorical Interactions
  
## Factorial designs

- Used to address a question involving more than one factor that can influence a DV, with each factor acting alone *or in combination with other factors*

  - What are the effects of cognitive therapy and drug therapy on treating depression?
  - Do male and female students learn better with male or female
    teachers?

## Full Factorial Designs {.smaller}

- A study has a full factorial design if it has more than one IV and the levels of the IVs are "fully crossed" - every combination of factors is tested
- Designs are designated using RxC (row-by-column) format; two-by-two (below), two-by-three, three-by-three, etc.
- **cell:** unique combination of the levels of the factors ($A_1$$B_1$)

![](img/factorial.png){fig-align="center"}

## Factorial Plots and Interpretation

![](img/scenarios.png){fig-align="center"}

## Scenario 1

:::: {.columns}

::: {.column width="40%"}

![](img/scenarios1.png){fig-align="left"}

::: 

::: {.column width="60%"}

Main Effects

* Effect of Drug: Yes > No
* Effect of Therapy: Yes > No

Interaction:

* Did the effect of cognitive therapy depend on whether or not they got drug therapy? No
* Therapy increase is 20 for both Drug groups

:::

::::

## Scenario 2

:::: {.columns}

::: {.column width="40%"}

![](img/scenarios2.png){fig-align="left"}

::: 

::: {.column width="60%"}

Main Effects

* No Effect of Drug: Yes = No
* Effect of Therapy: Yes > No

Interaction:

* No
* Therapy increase is 20 for both Drug groups

:::

::::

## Scenario 3

:::: {.columns}

::: {.column width="40%"}

![](img/scenarios3.png){fig-align="left"}

::: 

::: {.column width="60%"}

Main Effects

* Effect of Drug: Yes > No
* Effect of Therapy: Yes > No

Interaction:

* Yes. Change is bigger in Yes-Cognitive-Yes-Drug from Yes-Cognitive-No-Drug (+30), than it is in No-Cognitive-Yes-Drug from No-Cognitive-No-Drug (+10)

:::

::::

## Terminology

:::: {.columns}

::: {.column width="40%"}

![](img/scenarios3.png){fig-align="left"}

::: 

::: {.column width="60%"}

- Main Effects: tests of **marginal means**
    - $H_0: \mu_{A_1} = \mu_{A_2}$
    - $H_0: \mu_{B_1} = \mu_{B_2}$

- Called marginal means because they are literally in the margin of the table - 75 vs 45, 70 vs 50
- The averages of the rows and columns 

:::

::::

## Terminology {.smaller}

:::: {.columns}

::: {.column width="40%"}

![](img/scenarios3.png){fig-align="left"}

::: 

::: {.column width="60%"}

- Simple Effects: effect of factor at level of other
    - eff of $B$ at $A_1$, $H_0: \mu_{A_1B_1} = \mu_{A_1B_2}$
    - eff of $B$ at $A_2$, $H_0: \mu_{A_2B_1} = \mu_{A_2B_2}$

- The effect of No Cognitive therapy vs Yes Cognitive therapy at the level No Drug use, rises from 40 to 60
- The effect of No Cognitive therapy vs Yes Cognitive therapy at the level Yes Drug use, rises from 50 to 90
- The interaction is a comparison of the simple effects to see if they are equal!
    - $H_0: \mu_{A_1B_2}-\mu_{A_1B_1} = \mu_{A_2B_2}-\mu_{A_2B_1}$
- You can compare simple effects in any way you like, by column or by row.

:::

::::


## Effects in Factorial Designs

- Main Effects: tests of **marginal means**
    - $H_0: \mu_{A_1} = \mu_{A_2}$
    - $H_0: \mu_{B_1} = \mu_{B_2}$

- Simple Effects: effect of factor at level of other
    - eff of $B$ at $A_1$, $H_0: \mu_{A_1B_1} = \mu_{A_1B_2}$
    - eff of $B$ at $A_2$, $H_0: \mu_{A_2B_1} = \mu_{A_2B_2}$

- Interaction: equivalence of simple effects
    - $H_0: \mu_{A_1B_2}-\mu_{A_1B_1} = \mu_{A_2B_2}-\mu_{A_2B_1}$

## A fallacy

> "The percentage of neurons showing cue-related activity increased with training in the mutant mice ($p < 0.05$), but not in the control mice ($p > 0.05$)."

Saying the simple effect is significant in one case but not in another does not imply that the simple effects are statistically different (i.e. that there is an interaction)!

::: aside
Gelman, A., & Stern, H. (2012). [The difference between "significant" and "not significant" is not itself statistically significant.](https://amstat.tandfonline.com/doi/abs/10.1198/000313006X152649#.XbAog_fTXeQ) *The American Statistician*, *60*, 328--331.

Nieuwenhuis, S., Forstmann, B. U., & Wagenmakers, E. J. (2011). [Erroneous analyses of interactions in neuroscience: a problem of significance.](https://www.nature.com/articles/nn.2886?draft=collection) *Nature Neuroscience*, *14*, 1105-1107.
:::

## For info: {.smaller}

Note:

- Standard is to consider interaction first and then look at main effects. Some would not interpret main effects if there is an interaction.
- People are starting to get hesitant to go really beyond three-way interactions (three factors) as an interactions becomes really hard to determine what it means, as three-way are hard enough.
    - three-way: the effect of A1 vs A2 is contingent upon the level of B (B1 vs B2) and the level of C (C1 vs C2)
    - Something like, HGS is dependent on hand-preference, gym membership and eye color
    - E.g. blue eyed right-handed people with a gym membership have a higher HGS than brown eyed left-handed people without a gym membership


# Coding categorical predictors

---

::: {.r-stack}
```{r}
#| echo: false
tribble(~Coding, ~A_1, ~A_2, ~Total,~Dist,
	  "Treatment (dummy)", "$0$", "$1$", "$1$", "$1$",
	  "Sum",               "$-1$", "$1$", "$2$", "$2$",
	  "Deviation",
	  .p("$", "-", .f(1, 2), "$"),
	  .p("$", .f(1, 2), "$"),  "$1$", "$1$") |>
    knitr::kable(col.names = c("Scheme", "$A_1$", "$A_2$", "Total", "Dist"),
	  align = "lrrrr") ##|>
    ##  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = "striped")
```
:::

Choice of a coding scheme impacts interpretation of:

1. the intercept term; and
2. the interpretation of the tests for all but the highest-order effects and interactions in a factorial design.
    - highest order means the biggest interaction in your model
    
---

::: {.r-stack}
```{r}
#| echo: false
tribble(~Coding, ~A_1, ~A_2, ~Total,~Dist,
	  "Treatment (dummy)", "$0$", "$1$", "$1$", "$1$",
	  "Sum",               "$-1$", "$1$", "$2$", "$2$",
	  "Deviation",
	  .p("$", "-", .f(1, 2), "$"),
	  .p("$", .f(1, 2), "$"),  "$1$", "$1$") |>
    knitr::kable(col.names = c("Scheme", "$A_1$", "$A_2$", "Total", "Dist"),
	  align = "lrrrr") ##|>
    ##  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = "striped")
```
:::

Impact of Distance (Dist):

* when Distance of coding scheme is 1 then the coefficients of categorical predictors is basically the mean values
* when Distance of coding scheme is 2 then the coefficients of categorical predictors is half the mean value

---

::: {.r-stack}
```{r}
#| echo: false
tribble(~Coding, ~A_1, ~A_2, ~Total,~Dist,
	  "Treatment (dummy)", "$0$", "$1$", "$1$", "$1$",
	  "Sum",               "$-1$", "$1$", "$2$", "$2$",
	  "Deviation",
	  .p("$", "-", .f(1, 2), "$"),
	  .p("$", .f(1, 2), "$"),  "$1$", "$1$") |>
    knitr::kable(col.names = c("Scheme", "$A_1$", "$A_2$", "Total", "Dist"),
	  align = "lrrrr") ##|>
    ##  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = "striped")
```
:::

We recommend using Deviation in categorical by categorical:

* the main effects can be interpreted as marginal means
* when using dummy code, the coeffecients are basically simple effects.
* in short, deviation is easier to interpret


## {.smaller}

```{r}
#| echo: false
tribble(~term, ~deviation,
        "$\\mu$", 
        .yb1("..."),
        ## (A) second row
        "$A$",
        .p("$", .st(.yb2("2.."), .yb2("1..")), "$"),
        "$B$",
        .p("$", .st(.yb2(".2."), .yb2(".1.")), "$"),
        "$C$",
        .p("$", .st(.yb2("..2"), .yb2("..1")), "$"),
        "$AB$",
        .p("$", .st(.st(.yb2("22."), .yb2("12."), .rb),
                      .st(.yb2("21."), .yb2("11."), .rb)),
           "$"),
        "$AC$",
        .p("$", .st(.st(.yb2("2.2"), .yb2("1.2"), .rb),
                      .st(.yb2("2.1"), .yb2("1.1"), .rb)),
           "$"),
        "$BC$",
        .p("$", .st(.st(.yb2(".22"), .yb2(".12"), .rb),
                      .st(.yb2(".21"), .yb2(".11"), .rb)),
           "$")) |>
          knitr::kable(align = "cc", escape = FALSE)
# |>
#  knitr::kable(align = "cccc")  |>
#  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = "striped")
```

* coefficient mapping for three factor analysis with deviation
* $\bar{Y}_{...}$ is code for grand mean - mean of everything
* main effect of A, B, and C, interaction AB, AC, BC, interaction ABC

## {.smaller}

```{r}
#| echo: false
tribble(~term, ~treatment, ~sum, ~deviation,
        "$\\mu$", .yb1("111"), .yb1("..."), .yb1("..."),
        ## (A) second row
        "$A$",
        .p("$", .st(.yb2("211"), .yb2("111")), "$"),
        .p("$", .f(.st(.yb2("2.."), .yb2("1.."), .rb), 2), "$"),
        .p("$", .st(.yb2("2.."), .yb2("1..")), "$"),
        "$B$",
        .p("$", .st(.yb2("121"), .yb2("111")), "$"),
        .p("$", .f(.st(.yb2(".2."), .yb2(".1."), .rb), 2), "$"),
        .p("$", .st(.yb2(".2."), .yb2(".1.")), "$"),
        "$C$",
        .p("$", .st(.yb2("112"), .yb2("111")), "$"),
        .p("$", .f(.st(.yb2("..2"), .yb2("..1"), .rb), 2), "$"),
        .p("$", .st(.yb2("..2"), .yb2("..1")), "$"),
        "$AB$",
        .p("$", .st(.st(.yb2("221"), .yb2("121"), .rb),
                      .st(.yb2("211"), .yb2("111"), .rb)),
           "$"),
        .p("$", .f(.st(.st(.yb2("22."), .yb2("12."), .rb),
                         .st(.yb2("21."), .yb2("11."), .rb)), 4),
           "$"),
        .p("$", .st(.st(.yb2("22."), .yb2("12."), .rb),
                      .st(.yb2("21."), .yb2("11."), .rb)),
           "$"),
        "$AC$",
        .p("$", .st(.st(.yb2("212"), .yb2("211"), .rb),
                      .st(.yb2("112"), .yb2("111"), .rb)),
           "$"),
        .p("$", .f(.st(.st(.yb2("2.2"), .yb2("1.2"), .rb),
                         .st(.yb2("2.1"), .yb2("1.1"), .rb)), 4),
           "$"),
        .p("$", .st(.st(.yb2("2.2"), .yb2("1.2"), .rb),
                      .st(.yb2("2.1"), .yb2("1.1"), .rb)),
           "$"),
        "$BC$",
        .p("$", .st(.st(.yb2("122"), .yb2("112"), .rb),
                      .st(.yb2("121"), .yb2("111"), .rb)),
           "$"),
        .p("$", .f(.st(.st(.yb2(".22"), .yb2(".12"), .rb),
                         .st(.yb2(".21"), .yb2(".11"), .rb)), 4),
           "$"),
        .p("$", .st(.st(.yb2(".22"), .yb2(".12"), .rb),
                      .st(.yb2(".21"), .yb2(".11"), .rb)),
           "$")) |>
          knitr::kable(align = "cccc", escape = FALSE)
# |>
#  knitr::kable(align = "cccc")  |>
#  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = "striped")
```

* coefficient mapping for a three factor analysis for all coding schemes
* $\bar{Y}_{...}$ is code for grand mean - mean of everything
* main effect of A, B, and C, interaction AB, AC, BC, interaction ABC

## More than 2 levels ($k > 2$)?

Arbitrarily choose one as "baseline".

:::: {.panel-tabset}

### Deviation ($k = 3$)

:::: {.panel-tabset}

#### scheme

:::: {.columns}

::: {.column width="50%"}

|       |    `A2v1`      | `A3v1`         |
|:------|---------------:|---------------:|
| $A_1$ | $-\frac{1}{3}$ | $-\frac{1}{3}$ |
| $A_2$ | $\frac{2}{3}$  | $-\frac{1}{3}$ |
| $A_3$ | $-\frac{1}{3}$ | $\frac{2}{3}$  |

:::

:::  {.column width="50%"}


| `target` |` otherwise`|
|:--------:|:----------:|
| $\frac{k - 1}{k}$        |$-\frac{1}{k}$          |

$k$ = number of conditions in that factor

:::

::::

#### code

target level: $\frac{k - 1}{k}$, otherwise: $-\frac{1}{k}$

```{r}
#| eval: false
dat |>
  mutate(A2v1 = if_else(A == "A2", 2/3, -1/3),
         A3v1 = if_else(A == "A3", 2/3, -1/3))
```

::::



### Dummy ($k = 3$)

:::: {.panel-tabset}

#### scheme

:::: {.columns}

::: {.column width="50%"}

|       | `A2v1` | `A3v1` | 
|:------|-------:|-------:| 
| $A_1$ |      0 |      0 | 
| $A_2$ |      1 |      0 |
| $A_3$ |      0 |      1 |

:::

:::  {.column width="50%"}


| `target` |` otherwise`|
|:--------:|:----------:|
| 1        |0           |

:::

::::


#### code

target level: 1, otherwise: 0
```{r}
#| eval: false
dat |>
  mutate(A2v1 = if_else(A == "A2", 1, 0),
         A3v1 = if_else(A == "A3", 1, 0))
```


::::

::::

## Linear Model Formulas in R

* Assuming a three-way factorial analysis with two levels per factor (2 * 2 * 2)

::: {.r-stack}
`y ~ a * b * c`
:::

::: {.r-stack}
is shorthand for
:::

::: {.r-stack}
`y ~ a + b + c + a:b + a:c + b:c + a:b:c`
:::

* a, b and c are main effects
* a:b, a:c, and b:c are two-way interactions
* a:b:c is the three-way (highest order) interaction

---

* Assuming a two-way factorial analysis with three levels in A and two levels in B (3 * 2)

::: {.r-stack}
when factor $A$ has 3 levels: 
:::

::: {.r-stack}
`y ~ (a1 + a2) * b`
:::

::: {.r-stack}
is shorthand for
:::

::: {.r-stack}
`y ~ a1 + a2 + b + a1:b + a2:b`
:::

* a1 and a2 combined are main effect of a
* a1:b and a2:b combined are interaction of a by b

---

* Assuming a two-way factorial analysis with three levels in each (3 * 3)

::: {.r-stack}
when $A$ and $B$ have 3 levels:
:::

::: {.r-stack}
`y ~ (a1 + a2) * (b1 + b2)`
:::

::: {.r-stack}
is shorthand for
:::

::: {.r-stack}
`y ~ a1 + a2 + b1 + b2 + a1:b1 + a1:b2 + a2:b1 + a2:b2`
:::

* a1 and a2 combined are main effect of a
* b1 and b2 combined are main effect of b
* a1:b1, a1:b2, a2:b1 and a2:b2 combined are interaction of a and b

# Activities & Next Week

Activities:

* Formative Data Task on Moodle
* Chapter 4 of the Book

Next Week:

* Multi-level modelling 
    * more than one response per participant
