{
  "hash": "7b4833ecdcbcb215555c19d58c15a2de",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stat Models (09): Confirmatory factor analysis (CFA)\"\nauthor: \"Dale Barr\"\ninstitute: University of Glasgow\ntitle-slide-attributes:\n  data-background-image: \"img/titlescreen.png\"\nformat: \n  revealjs:\n    theme: dark \n    code-line-numbers: false\n    chalkboard: true\n    df-print: tibble\nknitr:\n  opts_chunk:\n    echo: true\n---\n\n\n\n\n\n## part II\n\n| lecture | topic                                                 |\n|---------|-------------------------------------------------------|\n| 6       | introduction to multivariate analysis                 |\n| 7       | path analysis                                         |\n| 8       | mediation models                                      |\n| 9       | [confirmatory factor analysis]{style=\"color:yellow;\"} |\n| 10      | structural equation modeling                          |\n\n## what is CFA? {.smaller}\n\n> \"a type of SEM that deals specifically with measurement models—that is, the relationships between observed measures or *indicators* (e.g., test items, test scores, behavioral observation ratings) and latent variables or *factors*.\" \n\n- hypothesis-driven, not data-driven\n  - vs *exploratory factor analysis* (EFA), principal components analysis (*PCA*)\n\n::: {.aside}\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research (2nd edition). New York: Guilford Press.\n\n:::\n\n## when is it useful? \n\n- later stages of scale development (vs EFA)\n- psychometric evaluation of test instruments\n- full SEM\n  - forms the 'measurement' part of the model (vs. structural)\n\n## psychometric evaluation\n\n- estimate scale reliability (vs Cronbach's alpha)\n- *convergent* & *discriminant* validity adjusting for measurement error\n- measurement invariance (multigroup CFA)\n- accounting for \"method\" effects\n  - additional non-factor covariation introduced by the measurement approach\n\n## Holzinger & Swineford {.smaller}\n\n| variable | description                                                      |\n|----------|------------------------------------------------------------------|\n| `x1`     | Visual perception test from Spearman VPT Part I                  |\n| `x2`     | Cubes, Simplification of Brighams Spatial Relations Test         |\n| `x3`     | Lozenges from Thorndike-Shapes flipped over then identify target |\n| `x4`     | Paragraph comprehension                                          |\n| `x5`     | Sentence completion                                              |\n| `x6`     | Word meaning                                                     |\n| `x7`     | Speeded addition                                                 |\n| `x8`     | Speeded counting of dots                                         |\n| `x9`     | Speeded discrimination of straight and curved capitals           |\n\n::: {.aside}\n\nHolzinger, K. J., & Swineford, F. (1939). A study in factor analysis: The stability of a bi-factor solution. *Supplementary Educational Monographs*, xi + 91.\n\n:::\n\n## Holzinger & Swineford data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"lavaan\")\nlibrary(\"tidyverse\")\n\nHolzingerSwineford1939 |>\n  as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 301 × 15\n      id   sex ageyr agemo school  grade    x1    x2    x3    x4    x5    x6\n   <int> <int> <int> <int> <fct>   <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     1     1    13     1 Pasteur     7  3.33  7.75 0.375  2.33  5.75 1.29 \n 2     2     2    13     7 Pasteur     7  5.33  5.25 2.12   1.67  3    1.29 \n 3     3     2    13     1 Pasteur     7  4.5   5.25 1.88   1     1.75 0.429\n 4     4     1    13     2 Pasteur     7  5.33  7.75 3      2.67  4.5  2.43 \n 5     5     2    12     2 Pasteur     7  4.83  4.75 0.875  2.67  4    2.57 \n 6     6     2    14     1 Pasteur     7  5.33  5    2.25   1     3    0.857\n 7     7     1    12     1 Pasteur     7  2.83  6    1      3.33  6    2.86 \n 8     8     2    12     2 Pasteur     7  5.67  6.25 1.88   3.67  4.25 1.29 \n 9     9     2    13     0 Pasteur     7  4.5   5.75 1.5    2.67  5.75 2.71 \n10    11     2    12     5 Pasteur     7  3.5   5.25 0.75   2.67  5    2.57 \n# ℹ 291 more rows\n# ℹ 3 more variables: x7 <dbl>, x8 <dbl>, x9 <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Holzinger & Swineford data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHolzingerSwineford1939 |>\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id             sex            ageyr        agemo       \n Min.   :  1.0   Min.   :1.000   Min.   :11   Min.   : 0.000  \n 1st Qu.: 82.0   1st Qu.:1.000   1st Qu.:12   1st Qu.: 2.000  \n Median :163.0   Median :2.000   Median :13   Median : 5.000  \n Mean   :176.6   Mean   :1.515   Mean   :13   Mean   : 5.375  \n 3rd Qu.:272.0   3rd Qu.:2.000   3rd Qu.:14   3rd Qu.: 8.000  \n Max.   :351.0   Max.   :2.000   Max.   :16   Max.   :11.000  \n                                                              \n         school        grade             x1               x2       \n Grant-White:145   Min.   :7.000   Min.   :0.6667   Min.   :2.250  \n Pasteur    :156   1st Qu.:7.000   1st Qu.:4.1667   1st Qu.:5.250  \n                   Median :7.000   Median :5.0000   Median :6.000  \n                   Mean   :7.477   Mean   :4.9358   Mean   :6.088  \n                   3rd Qu.:8.000   3rd Qu.:5.6667   3rd Qu.:6.750  \n                   Max.   :8.000   Max.   :8.5000   Max.   :9.250  \n                   NA's   :1                                       \n       x3              x4              x5              x6        \n Min.   :0.250   Min.   :0.000   Min.   :1.000   Min.   :0.1429  \n 1st Qu.:1.375   1st Qu.:2.333   1st Qu.:3.500   1st Qu.:1.4286  \n Median :2.125   Median :3.000   Median :4.500   Median :2.0000  \n Mean   :2.250   Mean   :3.061   Mean   :4.341   Mean   :2.1856  \n 3rd Qu.:3.125   3rd Qu.:3.667   3rd Qu.:5.250   3rd Qu.:2.7143  \n Max.   :4.500   Max.   :6.333   Max.   :7.000   Max.   :6.1429  \n                                                                 \n       x7              x8               x9       \n Min.   :1.304   Min.   : 3.050   Min.   :2.778  \n 1st Qu.:3.478   1st Qu.: 4.850   1st Qu.:4.750  \n Median :4.087   Median : 5.500   Median :5.417  \n Mean   :4.186   Mean   : 5.527   Mean   :5.374  \n 3rd Qu.:4.913   3rd Qu.: 6.100   3rd Qu.:6.083  \n Max.   :7.435   Max.   :10.000   Max.   :9.250  \n                                                 \n```\n\n\n:::\n:::\n\n\n\n## CFA diagram {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](img/hs1939.png){fig-align=\"center\" width=\"65%\"}\n\n:::\n\n::: {.column width=\"50%\"} \n\n- account for measurement error\n- unidimensional measurement\n  - \"loadings\"\n- \"reflective\" measurement (vs. formative)\n- common vs. unique variance\n\n:::\n\n::::\n\n## model identification\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](img/hs1939.png){fig-align=\"center\" width=\"65%\"}\n\n:::\n\n::: {.column width=\"50%\"} \n\n- single factor: >2 X vars\n- two factors: $\\ge$ 2 Xs per F\n\n- latent var identification:\n  1. reference indicator\n  2. unit variance\n\n:::\n\n::::\n\n## estimation in lavaan {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](img/hs1939_unstandardized.png){fig-align=\"center\"}\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"lavaan\")\n\nmod_hs <- '\nvisual  =~ x1 + x2 + x3\ntextual =~ x4 + x5 + x6\nspeed   =~ x7 + x8 + x9\n'\n\nfit_hs <- cfa(mod_hs, data = HolzingerSwineford1939)\n\nsummary(fit_hs, fit.measures = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlavaan 0.6-19 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                85.306\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               918.852\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.931\n  Tucker-Lewis Index (TLI)                       0.896\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3737.745\n  Loglikelihood unrestricted model (H1)      -3695.092\n                                                      \n  Akaike (AIC)                                7517.490\n  Bayesian (BIC)                              7595.339\n  Sample-size adjusted Bayesian (SABIC)       7528.739\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.092\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.114\n  P-value H_0: RMSEA <= 0.050                    0.001\n  P-value H_0: RMSEA >= 0.080                    0.840\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.065\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.554    0.100    5.554    0.000\n    x3                0.729    0.109    6.685    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5                1.113    0.065   17.014    0.000\n    x6                0.926    0.055   16.703    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.180    0.165    7.152    0.000\n    x9                1.082    0.151    7.155    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~~                                           \n    textual           0.408    0.074    5.552    0.000\n    speed             0.262    0.056    4.660    0.000\n  textual ~~                                          \n    speed             0.173    0.049    3.518    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                0.549    0.114    4.833    0.000\n   .x2                1.134    0.102   11.146    0.000\n   .x3                0.844    0.091    9.317    0.000\n   .x4                0.371    0.048    7.779    0.000\n   .x5                0.446    0.058    7.642    0.000\n   .x6                0.356    0.043    8.277    0.000\n   .x7                0.799    0.081    9.823    0.000\n   .x8                0.488    0.074    6.573    0.000\n   .x9                0.566    0.071    8.003    0.000\n    visual            0.809    0.145    5.564    0.000\n    textual           0.979    0.112    8.737    0.000\n    speed             0.384    0.086    4.451    0.000\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n## model measures of fit {.smaller}\n\n| fit measure                                     | heuristic |\n|-------------------------------------------------|-----------|\n| $X^2$ $p$-value                                 | $>.05$    |\n| Root Mean Square Error of Approximation (RMSEA) | $<.08$    |\n| Comparative Fit Index (CFI)                     | $>.95$    |\n| Standardized Root Mean Square Residual (SRMR)   | $<.08$    |\n\n- how can we improve the fit?\n  - allow cross loadings\n  - allow indicator covariance\n\n## `modificationIndices()` {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodificationIndices(fit_hs) |>\n  as_tibble() |>\n  print(n = +Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 54 × 8\n   lhs     op    rhs         mi      epc  sepc.lv sepc.all sepc.nox\n   <chr>   <chr> <chr>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1 visual  =~    x4     1.21     0.0766   0.0689   0.0593   0.0593 \n 2 visual  =~    x5     7.44    -0.210   -0.189   -0.147   -0.147  \n 3 visual  =~    x6     2.84     0.111    0.100    0.0916   0.0916 \n 4 visual  =~    x7    18.6     -0.422   -0.380   -0.349   -0.349  \n 5 visual  =~    x8     4.29    -0.210   -0.189   -0.187   -0.187  \n 6 visual  =~    x9    36.4      0.577    0.519    0.515    0.515  \n 7 textual =~    x1     8.90     0.350    0.347    0.297    0.297  \n 8 textual =~    x2     0.0173  -0.0114  -0.0112  -0.00956 -0.00956\n 9 textual =~    x3     9.15    -0.272   -0.269   -0.238   -0.238  \n10 textual =~    x7     0.0979  -0.0210  -0.0208  -0.0191  -0.0191 \n11 textual =~    x8     3.36    -0.121   -0.120   -0.118   -0.118  \n12 textual =~    x9     4.80     0.138    0.137    0.136    0.136  \n13 speed   =~    x1     0.0139   0.0244   0.0151   0.0130   0.0130 \n14 speed   =~    x2     1.58    -0.198   -0.123   -0.105   -0.105  \n15 speed   =~    x3     0.716    0.136    0.0842   0.0746   0.0746 \n16 speed   =~    x4     0.00326 -0.00506 -0.00314 -0.00270 -0.00270\n17 speed   =~    x5     0.201   -0.0440  -0.0272  -0.0211  -0.0211 \n18 speed   =~    x6     0.273    0.0441   0.0273   0.0250   0.0250 \n19 x1      ~~    x2     3.61    -0.184   -0.184   -0.233   -0.233  \n20 x1      ~~    x3     0.935   -0.139   -0.139   -0.203   -0.203  \n21 x1      ~~    x4     3.55     0.0783   0.0783   0.173    0.173  \n22 x1      ~~    x5     0.522   -0.0332  -0.0332  -0.0670  -0.0670 \n23 x1      ~~    x6     0.0482   0.00870  0.00870  0.0197   0.0197 \n24 x1      ~~    x7     5.42    -0.129   -0.129   -0.195   -0.195  \n25 x1      ~~    x8     0.634   -0.0411  -0.0411  -0.0793  -0.0793 \n26 x1      ~~    x9     7.33     0.138    0.138    0.247    0.247  \n27 x2      ~~    x3     8.53     0.218    0.218    0.223    0.223  \n28 x2      ~~    x4     0.534   -0.0338  -0.0338  -0.0521  -0.0521 \n29 x2      ~~    x5     0.0226  -0.00767 -0.00767 -0.0108  -0.0108 \n30 x2      ~~    x6     0.785    0.0392   0.0392   0.0617   0.0617 \n31 x2      ~~    x7     8.92    -0.183   -0.183   -0.192   -0.192  \n32 x2      ~~    x8     0.0535  -0.0125  -0.0125  -0.0168  -0.0168 \n33 x2      ~~    x9     1.89     0.0751   0.0751   0.0937   0.0937 \n34 x3      ~~    x4     0.142   -0.0158  -0.0158  -0.0282  -0.0282 \n35 x3      ~~    x5     7.86    -0.130   -0.130   -0.212   -0.212  \n36 x3      ~~    x6     1.85     0.0548   0.0548   0.0999   0.0999 \n37 x3      ~~    x7     0.638   -0.0445  -0.0445  -0.0542  -0.0542 \n38 x3      ~~    x8     0.0589  -0.0121  -0.0121  -0.0188  -0.0188 \n39 x3      ~~    x9     4.13     0.102    0.102    0.147    0.147  \n40 x4      ~~    x5     2.53     0.186    0.186    0.457    0.457  \n41 x4      ~~    x6     6.22    -0.235   -0.235   -0.646   -0.646  \n42 x4      ~~    x7     5.92     0.0982   0.0982   0.180    0.180  \n43 x4      ~~    x8     3.80    -0.0691  -0.0691  -0.162   -0.162  \n44 x4      ~~    x9     0.196   -0.0159  -0.0159  -0.0347  -0.0347 \n45 x5      ~~    x6     0.916    0.101    0.101    0.253    0.253  \n46 x5      ~~    x7     1.23    -0.0495  -0.0495  -0.0828  -0.0828 \n47 x5      ~~    x8     0.347    0.0230   0.0230   0.0493   0.0493 \n48 x5      ~~    x9     0.999    0.0396   0.0396   0.0788   0.0788 \n49 x6      ~~    x7     0.259   -0.0197  -0.0197  -0.0368  -0.0368 \n50 x6      ~~    x8     0.275    0.0178   0.0178   0.0426   0.0426 \n51 x6      ~~    x9     0.0971  -0.0107  -0.0107  -0.0239  -0.0239 \n52 x7      ~~    x8    34.1      0.536    0.536    0.859    0.859  \n53 x7      ~~    x9     5.18    -0.187   -0.187   -0.278   -0.278  \n54 x8      ~~    x9    14.9     -0.423   -0.423   -0.805   -0.805  \n```\n\n\n:::\n:::\n\n\n\n## `modificationIndices()` {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodificationIndices(fit_hs) |>\n  as_tibble() |>\n  arrange(desc(mi)) |>\n  print(n = +Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 54 × 8\n   lhs     op    rhs         mi      epc  sepc.lv sepc.all sepc.nox\n   <chr>   <chr> <chr>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1 visual  =~    x9    36.4      0.577    0.519    0.515    0.515  \n 2 x7      ~~    x8    34.1      0.536    0.536    0.859    0.859  \n 3 visual  =~    x7    18.6     -0.422   -0.380   -0.349   -0.349  \n 4 x8      ~~    x9    14.9     -0.423   -0.423   -0.805   -0.805  \n 5 textual =~    x3     9.15    -0.272   -0.269   -0.238   -0.238  \n 6 x2      ~~    x7     8.92    -0.183   -0.183   -0.192   -0.192  \n 7 textual =~    x1     8.90     0.350    0.347    0.297    0.297  \n 8 x2      ~~    x3     8.53     0.218    0.218    0.223    0.223  \n 9 x3      ~~    x5     7.86    -0.130   -0.130   -0.212   -0.212  \n10 visual  =~    x5     7.44    -0.210   -0.189   -0.147   -0.147  \n11 x1      ~~    x9     7.33     0.138    0.138    0.247    0.247  \n12 x4      ~~    x6     6.22    -0.235   -0.235   -0.646   -0.646  \n13 x4      ~~    x7     5.92     0.0982   0.0982   0.180    0.180  \n14 x1      ~~    x7     5.42    -0.129   -0.129   -0.195   -0.195  \n15 x7      ~~    x9     5.18    -0.187   -0.187   -0.278   -0.278  \n16 textual =~    x9     4.80     0.138    0.137    0.136    0.136  \n17 visual  =~    x8     4.29    -0.210   -0.189   -0.187   -0.187  \n18 x3      ~~    x9     4.13     0.102    0.102    0.147    0.147  \n19 x4      ~~    x8     3.80    -0.0691  -0.0691  -0.162   -0.162  \n20 x1      ~~    x2     3.61    -0.184   -0.184   -0.233   -0.233  \n21 x1      ~~    x4     3.55     0.0783   0.0783   0.173    0.173  \n22 textual =~    x8     3.36    -0.121   -0.120   -0.118   -0.118  \n23 visual  =~    x6     2.84     0.111    0.100    0.0916   0.0916 \n24 x4      ~~    x5     2.53     0.186    0.186    0.457    0.457  \n25 x2      ~~    x9     1.89     0.0751   0.0751   0.0937   0.0937 \n26 x3      ~~    x6     1.85     0.0548   0.0548   0.0999   0.0999 \n27 speed   =~    x2     1.58    -0.198   -0.123   -0.105   -0.105  \n28 x5      ~~    x7     1.23    -0.0495  -0.0495  -0.0828  -0.0828 \n29 visual  =~    x4     1.21     0.0766   0.0689   0.0593   0.0593 \n30 x5      ~~    x9     0.999    0.0396   0.0396   0.0788   0.0788 \n31 x1      ~~    x3     0.935   -0.139   -0.139   -0.203   -0.203  \n32 x5      ~~    x6     0.916    0.101    0.101    0.253    0.253  \n33 x2      ~~    x6     0.785    0.0392   0.0392   0.0617   0.0617 \n34 speed   =~    x3     0.716    0.136    0.0842   0.0746   0.0746 \n35 x3      ~~    x7     0.638   -0.0445  -0.0445  -0.0542  -0.0542 \n36 x1      ~~    x8     0.634   -0.0411  -0.0411  -0.0793  -0.0793 \n37 x2      ~~    x4     0.534   -0.0338  -0.0338  -0.0521  -0.0521 \n38 x1      ~~    x5     0.522   -0.0332  -0.0332  -0.0670  -0.0670 \n39 x5      ~~    x8     0.347    0.0230   0.0230   0.0493   0.0493 \n40 x6      ~~    x8     0.275    0.0178   0.0178   0.0426   0.0426 \n41 speed   =~    x6     0.273    0.0441   0.0273   0.0250   0.0250 \n42 x6      ~~    x7     0.259   -0.0197  -0.0197  -0.0368  -0.0368 \n43 speed   =~    x5     0.201   -0.0440  -0.0272  -0.0211  -0.0211 \n44 x4      ~~    x9     0.196   -0.0159  -0.0159  -0.0347  -0.0347 \n45 x3      ~~    x4     0.142   -0.0158  -0.0158  -0.0282  -0.0282 \n46 textual =~    x7     0.0979  -0.0210  -0.0208  -0.0191  -0.0191 \n47 x6      ~~    x9     0.0971  -0.0107  -0.0107  -0.0239  -0.0239 \n48 x3      ~~    x8     0.0589  -0.0121  -0.0121  -0.0188  -0.0188 \n49 x2      ~~    x8     0.0535  -0.0125  -0.0125  -0.0168  -0.0168 \n50 x1      ~~    x6     0.0482   0.00870  0.00870  0.0197   0.0197 \n51 x2      ~~    x5     0.0226  -0.00767 -0.00767 -0.0108  -0.0108 \n52 textual =~    x2     0.0173  -0.0114  -0.0112  -0.00956 -0.00956\n53 speed   =~    x1     0.0139   0.0244   0.0151   0.0130   0.0130 \n54 speed   =~    x4     0.00326 -0.00506 -0.00314 -0.00270 -0.00270\n```\n\n\n:::\n:::\n\n\n\n## `modificationIndices()` {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodificationIndices(fit_hs) |>\n  as_tibble() |>\n  filter(op == \"~~\") |>\n  arrange(desc(mi)) |>\n  print(n = +Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 36 × 8\n   lhs   op    rhs        mi      epc  sepc.lv sepc.all sepc.nox\n   <chr> <chr> <chr>   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1 x7    ~~    x8    34.1     0.536    0.536     0.859    0.859 \n 2 x8    ~~    x9    14.9    -0.423   -0.423    -0.805   -0.805 \n 3 x2    ~~    x7     8.92   -0.183   -0.183    -0.192   -0.192 \n 4 x2    ~~    x3     8.53    0.218    0.218     0.223    0.223 \n 5 x3    ~~    x5     7.86   -0.130   -0.130    -0.212   -0.212 \n 6 x1    ~~    x9     7.33    0.138    0.138     0.247    0.247 \n 7 x4    ~~    x6     6.22   -0.235   -0.235    -0.646   -0.646 \n 8 x4    ~~    x7     5.92    0.0982   0.0982    0.180    0.180 \n 9 x1    ~~    x7     5.42   -0.129   -0.129    -0.195   -0.195 \n10 x7    ~~    x9     5.18   -0.187   -0.187    -0.278   -0.278 \n11 x3    ~~    x9     4.13    0.102    0.102     0.147    0.147 \n12 x4    ~~    x8     3.80   -0.0691  -0.0691   -0.162   -0.162 \n13 x1    ~~    x2     3.61   -0.184   -0.184    -0.233   -0.233 \n14 x1    ~~    x4     3.55    0.0783   0.0783    0.173    0.173 \n15 x4    ~~    x5     2.53    0.186    0.186     0.457    0.457 \n16 x2    ~~    x9     1.89    0.0751   0.0751    0.0937   0.0937\n17 x3    ~~    x6     1.85    0.0548   0.0548    0.0999   0.0999\n18 x5    ~~    x7     1.23   -0.0495  -0.0495   -0.0828  -0.0828\n19 x5    ~~    x9     0.999   0.0396   0.0396    0.0788   0.0788\n20 x1    ~~    x3     0.935  -0.139   -0.139    -0.203   -0.203 \n21 x5    ~~    x6     0.916   0.101    0.101     0.253    0.253 \n22 x2    ~~    x6     0.785   0.0392   0.0392    0.0617   0.0617\n23 x3    ~~    x7     0.638  -0.0445  -0.0445   -0.0542  -0.0542\n24 x1    ~~    x8     0.634  -0.0411  -0.0411   -0.0793  -0.0793\n25 x2    ~~    x4     0.534  -0.0338  -0.0338   -0.0521  -0.0521\n26 x1    ~~    x5     0.522  -0.0332  -0.0332   -0.0670  -0.0670\n27 x5    ~~    x8     0.347   0.0230   0.0230    0.0493   0.0493\n28 x6    ~~    x8     0.275   0.0178   0.0178    0.0426   0.0426\n29 x6    ~~    x7     0.259  -0.0197  -0.0197   -0.0368  -0.0368\n30 x4    ~~    x9     0.196  -0.0159  -0.0159   -0.0347  -0.0347\n31 x3    ~~    x4     0.142  -0.0158  -0.0158   -0.0282  -0.0282\n32 x6    ~~    x9     0.0971 -0.0107  -0.0107   -0.0239  -0.0239\n33 x3    ~~    x8     0.0589 -0.0121  -0.0121   -0.0188  -0.0188\n34 x2    ~~    x8     0.0535 -0.0125  -0.0125   -0.0168  -0.0168\n35 x1    ~~    x6     0.0482  0.00870  0.00870   0.0197   0.0197\n36 x2    ~~    x5     0.0226 -0.00767 -0.00767  -0.0108  -0.0108\n```\n\n\n:::\n:::\n\n\n\n## updating the model {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](img/hs1939_unstandardized_updated.png){fig-align=\"center\"}\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_hs2 <- '\nvisual  =~ x1 + x2 + x3\ntextual =~ x4 + x5 + x6\nspeed   =~ x7 + x8 + x9\n\n## suggested by modificationIndices()\nx7 ~~ x8\n'\n\nfit_hs2 <- cfa(mod_hs2, data = HolzingerSwineford1939)\n\nsummary(fit_hs2, fit.measures = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlavaan 0.6-19 ended normally after 43 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                53.272\n  Degrees of freedom                                23\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               918.852\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.966\n  Tucker-Lewis Index (TLI)                       0.946\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3721.728\n  Loglikelihood unrestricted model (H1)      -3695.092\n                                                      \n  Akaike (AIC)                                7487.457\n  Bayesian (BIC)                              7569.013\n  Sample-size adjusted Bayesian (SABIC)       7499.242\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.066\n  90 Percent confidence interval - lower         0.043\n  90 Percent confidence interval - upper         0.090\n  P-value H_0: RMSEA <= 0.050                    0.118\n  P-value H_0: RMSEA >= 0.080                    0.175\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.047\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.576    0.098    5.898    0.000\n    x3                0.752    0.103    7.289    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5                1.115    0.066   17.015    0.000\n    x6                0.926    0.056   16.682    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.244    0.194    6.414    0.000\n    x9                2.515    0.641    3.924    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .x7 ~~                                               \n   .x8                0.353    0.067    5.239    0.000\n  visual ~~                                           \n    textual           0.400    0.073    5.511    0.000\n    speed             0.184    0.054    3.423    0.001\n  textual ~~                                          \n    speed             0.102    0.036    2.854    0.004\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                0.576    0.101    5.678    0.000\n   .x2                1.122    0.100   11.171    0.000\n   .x3                0.832    0.087    9.552    0.000\n   .x4                0.372    0.048    7.791    0.000\n   .x5                0.444    0.058    7.600    0.000\n   .x6                0.357    0.043    8.287    0.000\n   .x7                1.036    0.090   11.501    0.000\n   .x8                0.795    0.080    9.988    0.000\n   .x9                0.088    0.188    0.466    0.641\n    visual            0.783    0.135    5.810    0.000\n    textual           0.978    0.112    8.729    0.000\n    speed             0.147    0.056    2.615    0.009\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n## comparing models \n\n- use the `anova()` function (THIS IS NOT AN ANOVA!!) to compare the parent model to the nested model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fit_hs, fit_hs2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n     Df   AIC   BIC Chisq `Chisq diff`  RMSEA `Df diff`  `Pr(>Chisq)`\n  <int> <dbl> <dbl> <dbl>        <dbl>  <dbl>     <int>         <dbl>\n1    23 7487. 7569.  53.3         NA   NA            NA NA           \n2    24 7517. 7595.  85.3         32.0  0.321         1  0.0000000152\n```\n\n\n:::\n:::\n\n\n\n## standardizing latent variables {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](img/hs1939_updated_standardized-lv.png){fig-align=\"center\"}\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_hs2_stdlv <- cfa(mod_hs2, data = HolzingerSwineford1939,\n                     std.lv = TRUE)\n\nsummary(fit_hs2_stdlv, fit.measures = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlavaan 0.6-19 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                53.272\n  Degrees of freedom                                23\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               918.852\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.966\n  Tucker-Lewis Index (TLI)                       0.946\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3721.728\n  Loglikelihood unrestricted model (H1)      -3695.092\n                                                      \n  Akaike (AIC)                                7487.457\n  Bayesian (BIC)                              7569.013\n  Sample-size adjusted Bayesian (SABIC)       7499.242\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.066\n  90 Percent confidence interval - lower         0.043\n  90 Percent confidence interval - upper         0.090\n  P-value H_0: RMSEA <= 0.050                    0.118\n  P-value H_0: RMSEA >= 0.080                    0.175\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.047\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                0.885    0.076   11.620    0.000\n    x2                0.509    0.076    6.673    0.000\n    x3                0.665    0.072    9.238    0.000\n  textual =~                                          \n    x4                0.989    0.057   17.458    0.000\n    x5                1.103    0.063   17.601    0.000\n    x6                0.916    0.054   17.068    0.000\n  speed =~                                            \n    x7                0.383    0.073    5.229    0.000\n    x8                0.477    0.073    6.505    0.000\n    x9                0.963    0.106    9.051    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .x7 ~~                                               \n   .x8                0.353    0.067    5.239    0.000\n  visual ~~                                           \n    textual           0.457    0.064    7.142    0.000\n    speed             0.544    0.078    6.965    0.000\n  textual ~~                                          \n    speed             0.270    0.065    4.141    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                0.576    0.101    5.678    0.000\n   .x2                1.122    0.100   11.171    0.000\n   .x3                0.832    0.087    9.552    0.000\n   .x4                0.372    0.048    7.791    0.000\n   .x5                0.444    0.058    7.600    0.000\n   .x6                0.357    0.043    8.287    0.000\n   .x7                1.036    0.090   11.501    0.000\n   .x8                0.795    0.080    9.988    0.000\n   .x9                0.088    0.188    0.466    0.641\n    visual            1.000                           \n    textual           1.000                           \n    speed             1.000                           \n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n## {.smaller}\n\n![](img/higher-order-factor_dark.png){fig-align=\"center\"}\n\n::: {.aside}\n\nBeaujean, A. A., Parkin, J., & Parker, S. (2014). Comparing Cattell–Horn–Carroll factor models: Differences between bifactor and higher order factor models in predicting language achievement. *Psychological Assessment*, 26(3), 789–805. https://doi.org/10.1037/a0036745\n\n:::\n\n## MIMIC {.smaller}\n\n![](img/MIMIC_dark.png){fig-align=\"center\"}\n\n::: {.aside}\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research (2nd edition). New York: Guilford Press.\n\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}